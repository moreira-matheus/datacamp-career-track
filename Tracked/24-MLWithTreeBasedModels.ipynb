{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification-tree\n",
    "\n",
    "- Sequence of if-ealse questions about individual features\n",
    "- <u>Objective</u>: infer class labels\n",
    "- Able to capture non-linear relationships between features and labels\n",
    "- Don't require feature scaling\n",
    "\n",
    "## Decision-tree diagram\n",
    "\n",
    "<img src='decision-tree-diagram.PNG'>\n",
    "\n",
    "~~~\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "~~~\n",
    "\n",
    "## Decision regions\n",
    "\n",
    "- <u>Decision region</u>: region in the feature space where all instaces are assigned to one class label.\n",
    "- <u>Decision boundary</u>: surface separating different decision regions.\n",
    "\n",
    "## Building blocks\n",
    "\n",
    "- <u>Decision-Tree</u>: data structure consisting of a hierarchy of nodes\n",
    "- <u>Node</u>: question or prediction\n",
    "\t- Root: *no* parent node, question giving rise to *two* children nodes\n",
    "\t- Internal node: *one* parent node, question giving rise to *two* children nodes\n",
    "\t- Leaf: *one* parent node, *no* children nodes -> prediction\n",
    "\n",
    "## Information gain\n",
    "\n",
    "$$\n",
    "IG (f, sp) = I (\\textrm{parent}) - \\bigg( \\displaystyle\\frac{N_{\\textrm{left}}}{N} I(\\textrm{left}) + \\displaystyle\\frac{N_{\\textrm{right}}}{N} I(\\textrm{right}) \\bigg)\n",
    "$$\n",
    "\n",
    "where $f$ means feature and $sp$, split-point.\n",
    "\n",
    "- Criteria to measure impurity of a node $I(\\textrm{node})$:\n",
    "\t- gini index\n",
    "\t- entropy\n",
    "\n",
    "## Classification-tree learning\n",
    "\n",
    "- Nodes are grown recursively\n",
    "- At each node, split the data based on:\n",
    "\t- feature $f$ and split-point $sp$ to maximize $IG(\\textrm{node})$\n",
    "\t- if $IG (\\textrm{node}) = 0$, declare the node a leaf\n",
    "\n",
    "~~~\n",
    "dt = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "~~~\n",
    "\n",
    "## Regression-tree\n",
    "\n",
    "~~~\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=3)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "print(rmse_dt)\n",
    "~~~\n",
    "\n",
    "## Information criterion for Regression-tree\n",
    "\n",
    "$$\n",
    "I(\\textrm{node}) = \\underbrace{MSE(\\textrm{node})}_{\\textrm{mean-squared-error}} = \\displaystyle\\frac{1}{N_{\\textrm{node}}} \\displaystyle\\sum_{i\\ \\in\\ \\textrm{node}} \\big( y^{(i)} - \\hat y_{\\textrm{ node}} \\big)^2 \\\\\n",
    "\\underbrace{\\hat y_{\\textrm{node}}}_{\\textrm{mean-target-value}} = \\displaystyle\\frac{1}{N_{\\textrm{node}}} \\displaystyle\\sum_{i\\ \\in\\ \\textrm{node}} y^{(i)}\n",
    "$$\n",
    "\n",
    "<img src='./IMAGES/lin-reg-tree-reg.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning: Under the hood\n",
    "\n",
    "- Supervised learning: $y = f(x)$, $f$ is unknown\n",
    "\t- Find a model $\\hat f$ that best approximates $f$ : $\\hat f \\apporx f$\n",
    "\t- $\\hat f$ can be Logistic Regression, Decision Tree, Neural Network...\n",
    "\t- Discard noise as much as possible\n",
    "- <u>End goal</u>: $\\hat f$ should achieve a low predictive error on unseen data\n",
    "\n",
    "## Difficulties in approximating $f$\n",
    "\n",
    "- <u>Overfitting</u>: $\\hat f (x)$ fits the training set noise\n",
    "- <u>Undertraining</u>: $\\hat f$ is not flexible enough to approximate $f$\n",
    "\n",
    "## Generalization error\n",
    "\n",
    "- Does $\\hat f$ generalize well on unseen data?\n",
    "- It can be decomposed as follows: Generalization Error of $\\hat f = \\textrm{ bias}^2 + \\textrm{ variance } + \\textrm{ irreducible error}$\n",
    "\n",
    "- Bias: error term that tells you, on average, how much $\\hat f \\neq f$\n",
    "- Variance: tells you how much $\\hat f$ is inconsistent over different training sets\n",
    "\n",
    "- <u>Model complexity</u>: sets the flexibility of $\\hat f$\n",
    "\n",
    "<img src='./IMAGES/bias-variance-tradeoff.PNG'>\n",
    "\n",
    "## Estimating the Generalization Error\n",
    "\n",
    "- How do we estimate the generalization error of a model?\n",
    "\t- Cannot be done directly because:\n",
    "\t\t- $f$ is unknown\n",
    "\t\t- usually you only have one dataset\n",
    "\t\t- noise is unpredictable\n",
    "- Solution:\n",
    "\t- split the data to training and test sets\n",
    "\t- fit $\\hat f$ to the training set\n",
    "\t- evaluate the error of $\\hat f$ on the *unseen* test set\n",
    "\t- generalization error of $\\hat f \\approx$ test error of $\\hat f$\n",
    "\n",
    "## Better Model Evaluation with Cross-Validation (CV)\n",
    "\n",
    "- Test set should not be touched until we are confident the performance of $\\hat f$\n",
    "- Evaluating $\\hat f$ on training set: biased estimate, $\\hat f$ has already seen all training points\n",
    "\t- Solution: Cross-validation (CV)\n",
    "\t\t- K-Fold CV\n",
    "\t\t- Hold-out CV\n",
    "\n",
    "<img src='./IMAGES/kfold-cv.PNG'>\n",
    "\n",
    "$$\n",
    "\\textrm{CV error } = \\displaystyle\\frac{E_1 + ... + E_10}{10}\n",
    "$$\n",
    "\n",
    "## Diagnose variance problems\n",
    "\n",
    "- If $\\hat f$ suffers from <u>high variance</u>: CV error of $\\hat f >$ training set error of $\\hat f$ .\n",
    "\t- $\\hat f$ is said to overfit the training set.\n",
    "\t\t- decrease model complexity\n",
    "\t\t- for instance: decrease `max_depth`, increase `min_samples_leaf`\n",
    "\t\t- gather more data\n",
    "\n",
    "## Diagnose bias problems\n",
    "\n",
    "- If $\\hat f$ suffers from <u>high bias</u>: CV error of $\\hat f \\approx$ training set error of $\\hat f \\gg$ desired error\n",
    "\t- $\\hat f$ is said to underfit the training set\n",
    "\t\t- increase model complexity\n",
    "\t\t- for instance: increase `max_depth`, decrease `min_samples_leaf`\n",
    "\t\t- gather more relevant features\n",
    "\n",
    "## K-Fold CV in scikit-learn\n",
    "\n",
    "~~~\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=SEED)\n",
    "\n",
    "MSE_CV = cross_val_score(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = dt.predict(X_train)\n",
    "y_pred_test = dt.predict(X_test)\n",
    "~~~\n",
    "\n",
    "## Limitations of CARTs\n",
    "\n",
    "- Classification: can only produce orthogonal decision boundaries\n",
    "- Sensitive to small variations in the training set\n",
    "- High variance: unconstrained CARTs may overfit the training set\n",
    "- Solution: ensemble learning\n",
    "\n",
    "## Ensemble Learning\n",
    "\n",
    "- Train different models on the same dataset\n",
    "- Let each model make its predictions\n",
    "- Meta-model: aggregates predictions of individual models\n",
    "- Final prediction: more robust and less prone to errors\n",
    "- Best results: models are skillful in different ways\n",
    "\n",
    "<img src='./IMAGES/ensemble-learning.PNG'>\n",
    "\n",
    "## Voting classifier\n",
    "\n",
    "- Binary classification task\n",
    "- $N$ classifiers make predictions: $P_1, P_2, ..., P_N$ with $P_i \\in \\{0,1\\}$\n",
    "\n",
    "~~~\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "\t\t('K Nearest Neighbours', knn),\n",
    "\t\t('Classification Tree', dt)]\n",
    "\n",
    "for clf_name, clf in classifiers:\n",
    "\tclf.fit(X_train, y_train)\n",
    "\n",
    "\ty_pred = clf.predict(X_test)\n",
    "\n",
    "\tprint('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "print('Voting classifier: {.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "\n",
    "- Voting classifier: same training set, different algorithms\n",
    "- Bagging: one algorithm, different subsets of the training set\n",
    "\n",
    "## Bagging\n",
    "\n",
    "- stands for *Bootstrap Aggregation*\n",
    "- uses a technique known as bootstrap\n",
    "- reduces variance of individual models in the ensemble\n",
    "\n",
    "## Bagging: classification & regression\n",
    "\n",
    "- Classification:\n",
    "\t- Aggregates predictions by majority voting\n",
    "\t- `BaggingClassifier` in scikit-learn\n",
    "- Regression:\n",
    "\t- Aggregates predictions through averaging\n",
    "\t- `BaggingRegressor` in scikit-learn\n",
    "\n",
    "~~~\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimator=300, n_jobs=-1)\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "~~~\n",
    "\n",
    "## Out of Bag (OOB) instances\n",
    "\n",
    "- On average, for eahc model, 63% of the training instances are sampled\n",
    "- The remaining 37% constitute the OOB instances\n",
    "\n",
    "$$\n",
    "OOB_{\\textrm{score}} = \\displaystyle\\frac{OOB_1 + ... + OOB_N}{N}\n",
    "$$\n",
    "\n",
    "~~~\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_sample_leaf=0.16, random_state=SEED)\n",
    "\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimator=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "oob_accuracy = bc.oob_score_\n",
    "~~~\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "- Base estimator: decision tree\n",
    "- each estimator: trained on a different bootstrap sample having the same size as the training set\n",
    "- introduces further randomization in the training of individual trees\n",
    "- $d$ features are samples at each node without replacement ($d < $ total number of features)\n",
    "\n",
    "## RF: classification & regression\n",
    "\n",
    "- Classification:\n",
    "\t- Aggregates predictions by majority voting\n",
    "\t- `RandomForestClassifier` in scikit-learn\n",
    "- Regression\n",
    "\t- Aggregates predictions thorugh averaging\n",
    "\t- `RandomForestRegressor` in scikit-learn\n",
    "\n",
    "~~~\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "~~~\n",
    "\n",
    "## Feature importance\n",
    "\n",
    "Tree-based methods: enable measuring the importance of each feature in prediction.\n",
    "\n",
    "In `sklearn`:\n",
    "- how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
    "- accessed using the attribute `feature_importances_`\n",
    "\n",
    "~~~\n",
    "importance_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen')\n",
    "plt.show()\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "- Boosting: ensemble method combining several weak learners to form a strong learner\n",
    "- Weak learner: model doing slightly better than random guessing\n",
    "\t- Example: decision stump (CART whose `max_depth` is $1$)\n",
    "\n",
    "- Train an ensemble of predictors sequentially\n",
    "- Each predictor tries to correct its predecessor\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "- Stands for Adaptive Boosting\n",
    "- Each predictor pays morea attention to the instances wrongly predicted by its predecessor\n",
    "- Achieved by changing the weights of training instances\n",
    "\n",
    "<img src='./IMAGES/adaboost-training.PNG'>\n",
    "\n",
    "## AdaBoost: prediction\n",
    "\n",
    "- Classification\n",
    "\t- Weighted majority voting\n",
    "\t- In scikit-learn: `AdaBoostClassifier`\n",
    "- Regression:\n",
    "\t- Weighted average\n",
    "\t- In scikit-learn: `AdaBoostRegressor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbc = pd.read_csv('./DATASETS/wbc.csv', usecols=range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wbc.loc[:,'radius_mean':'fractal_dimension_worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wbc.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set probabilities of positive class\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GB)\n",
    "\n",
    "- Sequential correction of predecessor's errors\n",
    "- Does not tweak the weights of training instances\n",
    "- Each predictor is trained using the residual errors of its predecessor as labels\n",
    "- Gradient Boosted Trees: a CART is used as a base learner\n",
    "\n",
    "<img src='./IMAGES/gb-trees-training.PNG'>\n",
    "\n",
    "## GB Trees: prediction\n",
    "\n",
    "- Regression:\n",
    "\t- $y_{\\textrm{pred}} = y_1 + \\eta r_1 + ... + \\eta r_N$\n",
    "\t- In scikit-learn: `GradientBoostingRegressor`\n",
    "- Classification:\n",
    "\t- In scikit-learn: `GradientBoostingClassifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('./DATASETS/auto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>origin</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>US</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>Asia</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>Europe</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  origin  size\n",
       "0  18.0  250.0   88    3139   14.5      US  15.0\n",
       "1   9.0  304.0  193    4732   18.5      US  20.0\n",
       "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
       "3  18.5  250.0   98    3525   19.0      US  15.0\n",
       "4  34.3   97.0   78    2188   15.8  Europe  10.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = auto.drop(['mpg','origin'], axis=1)\n",
    "y = auto.mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 4.08\n"
     ]
    }
   ],
   "source": [
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting: cons\n",
    "\n",
    "- GB involves an exhaustive search procedure\n",
    "- Each CART is trained to find the best split points and features\n",
    "- May lead to CARTs using the same split points and maybe the same features\n",
    "\n",
    "## Stochastic Gradient Boosting (SGB)\n",
    "\n",
    "- each tree trained on a random subset of rows of the training dataset\n",
    "- the sampled instances (40%-80% of the training set) are sampled without replacement\n",
    "- features sampled (without replacement) when choosing split points\n",
    "- Result: further ensemble diversity\n",
    "- Effect: adding further variance to the ensemble of trees\n",
    "\n",
    "<img src='./IMAGES/sgb-training.PNG'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 4.28\n"
     ]
    }
   ],
   "source": [
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is hyperparameter tuning?\n",
    "\n",
    "- <u>Problem</u>: search for a set of optimal hyperparameters for a learning algorithm\n",
    "- <u>Solution</u>: find a set of optimal hyperparameters that results in an optimal model\n",
    "- <u>Optimal model</u>: yields an optimal score\n",
    "- <u>Score</u>: in scikit-learn defaults to accuracy (classification) and $R^2$ (regression)\n",
    "- Cross validation is used to estimate the generalization performance\n",
    "\n",
    "## Approaches to hyperparameter tuning\n",
    "\n",
    "- Grid search\n",
    "- Random search\n",
    "- Bayesian optimization\n",
    "- Genetic algorithms\n",
    "- ...\n",
    "\n",
    "## Grid search cross validation\n",
    "\n",
    "- Manually set a grid of discrete hyperparameter values\n",
    "- Set a metric for scoring model performance\n",
    "- Search exhaustively through the grid\n",
    "- For each set of hyperparameters, evaluate each model's CV score\n",
    "- The optimal hyperparameters are those of the model achieving the best CV score\n",
    "\n",
    "## Grid search: example\n",
    "- Hyperparameter grids:\n",
    "\t- `max_depth` = $\\{2,3,4\\}$\n",
    "\t- `min_samples_leaf` = $\\{0.05, 0.1\\}$\n",
    "- Hyperparameter space = $\\{(2, 0.05), (2, 0.1), (3, 0.05), ...\\}$\n",
    "- CV scores = $\\{\\textrm{score}_{(2, 0.05)}, ...\\}$\n",
    "- Optimal hyperparameters: set of hyperparameters corresponding to the best CV score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "print(dt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wbc.loc[:,'radius_mean':'fractal_dimension_worst']\n",
    "y = wbc.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: \n",
      " {'max_depth': 4, 'max_features': 0.4, 'min_samples_leaf': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_dt = {'max_depth': [3, 4, 5, 6], 'min_samples_leaf': [0.04, 0.06, 0.08], 'max_features': [0.2, 0.4, 0.6, 0.8]}\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "\n",
    "print('Best hyperparameters: \\n', best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of best model: 0.947\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "\n",
    "print('Test set accuracy of best model: {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = auto.drop(['mpg','origin'], axis=1)\n",
    "y = auto.mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   10.8s finished\n",
      "C:\\Users\\BigData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_estimators': [300, 400, 500], 'max_depth': [4, 6, 8], 'min_samples_leaf': [0.1, 0.2], 'max_features': ['log2', 'sqrt']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_rf = {'n_estimators': [300, 400, 500], 'max_depth': [4, 6, 8], \n",
    "             'min_samples_leaf': [0.1, 0.2], 'max_features': ['log2', 'sqrt']}\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: \n",
      " {'max_depth': 4, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams = grid_rf.best_params_\n",
    "\n",
    "print('Best hyperparameters: \\n', best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 3.92\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
